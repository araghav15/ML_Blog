[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\nprint(\"second blog\")\n\nsecond blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning How Machines Learn",
    "section": "",
    "text": "Unsupervised Learning\n\n\n\n\n\n\n\nunsupervised\n\n\nk-means\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nRaghav Agrawal\n\n\n\n\n\n\n  \n\n\n\n\nSupervised Learning\n\n\n\n\n\n\n\nml\n\n\nsupervised\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nRaghav Agrawal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised learning is a type of machine learning where an algorithm learns from a labeled dataset, which means it is provided with input-output pairs (or examples) during training. The goal of supervised learning is to learn a mapping from input data to the corresponding output or target variable, so that the algorithm can make predictions or classify new, unseen data based on the patterns it has learned from the training data. Supervised learning is widely used in various domains, including natural language processing, computer vision, recommendation systems, and healthcare, among others, to solve a wide range of problems where the relationship between input and output can be learned from labeled data.\nHere are some key characteristics of supervised learning:\n\nLabeled Data: In supervised learning, you have a dataset where each data point is associated with a known output. These labels provide the ground truth for the algorithm to learn from.\nTraining Phase: The algorithm goes through a training phase where it processes the labeled data to learn a model or function that can map input features to the correct output. The goal is to minimize the difference between the predicted outputs and the true labels.\nPrediction: After training, the model can be used to make predictions on new, unseen data. The model takes the input features of the new data and produces predictions or classifications based on what it has learned during training.\n\n\n\n\nSupervised Learning\n\n\n\n\n\nClassification: In classification , the goal is to assign input data to one of several predefined categories or classes. For example, spam email detection is a classification task where emails are classified as either spam or not spam.\nRegression: In regression tasks, the goal is to predict a continuous numerical value. For instance, predicting the price of a house based on its features like size, number of bedrooms, and location is a regression problem.\n\nCommon algorithms used in supervised learning include linear regression, logistic regression, decision trees, random forests, support vector machines, and various types of neural networks like feedforward neural networks and convolutional neural networks.\nIn this post, let’s dive deeper into how Logistic Regression works.\n\n\n\nDespite its name, logistic regression is used for classification, not regression. It is used for binary classification tasks, where the goal is to predict one of two possible outcomes or classes (usually represented as 0 and 1). In order to find predicted probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1.\n\n\n\n\n\nLet us take an example of how Logistic Regression works. In this example, we are going to work with a dataset that contains details of a bank customers and we will try to predict if a customer is eligible for a Credit Card or not.\nTo begin with, let us import the necessary Python modules and load the dataset.\n\n# Importing Necessary Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Load the Universal Bank Data\ndf = pd.read_csv('/home/raghav/Desktop/ML_Blog/posts/post-with-code/bank.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nAge\nExperience\nIncome\nZIP Code\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n1\n25\n1\n49\n91107\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n45\n19\n34\n90089\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n39\n15\n11\n94720\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n35\n9\n100\n94112\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n5\n35\n8\n45\n91330\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nBefore we train our model, we need to preprocess the data. Let us look if any feature has NULL values for all the data points.\n\ndf.isnull().sum()\n\nID                    0\nAge                   0\nExperience            0\nIncome                0\nZIP Code              0\nFamily                0\nCCAvg                 0\nEducation             0\nMortgage              0\nPersonal Loan         0\nSecurities Account    0\nCD Account            0\nOnline                0\nCreditCard            0\ndtype: int64\n\n\nSince no feature has NULL or zero values, let us find other features which might not contribute in prediction. ID and ZIP Code should not ideally determine if a person is eligible for Credit Card, so we should remove them from our dataset.\n\ndf1 = df.drop([\"ID\",\"ZIP Code\"], axis = 1)\ndf1.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n25\n1\n49\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n45\n19\n34\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n39\n15\n11\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n35\n9\n100\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n35\n8\n45\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nLet us now visualize the data and try to see the correlation between Income, Experience and having a credit card through a scatter plot.\n\nzero_class = df1[df1.CreditCard==0]\none_class = df1[df1.CreditCard==1]\n\n# Income vs Experience scatter plot\nplt.xlabel('Income')\nplt.ylabel('Experience')\nplt.scatter(zero_class['Income'],zero_class['Experience'], color = 'green', marker='+')\nplt.scatter(one_class['Income'], one_class['Experience'], color = 'red', marker='.')\n\n&lt;matplotlib.collections.PathCollection at 0x7fc0922ded50&gt;\n\n\n\n\n\nEven though the data is now clean, the values of different features are not comparable at all. Let us normalize them so that all features are brought to a similar scale or range without distorting the relative differences between their values.\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled = scaler.fit(df1.drop('CreditCard',axis=1)).transform(df1.drop('CreditCard',axis=1))\ndf_scaled = pd.DataFrame(scaled, columns=df1.columns[:-1])\ndf_scaled.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\n\n\n\n\n0\n-1.774417\n-1.666078\n-0.538229\n1.397414\n-0.193371\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n1\n-0.029524\n-0.096330\n-0.864109\n0.525991\n-0.250595\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n2\n-0.552992\n-0.445163\n-1.363793\n-1.216855\n-0.536720\n-1.049078\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n3\n-0.901970\n-0.968413\n0.569765\n-1.216855\n0.436103\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n4\n-0.901970\n-1.055621\n-0.625130\n1.397414\n-0.536720\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n\n\n\n\n\nLet us finally jump into building our model.\n\n# Create X and Y for the labeled data.\nx = df_scaled\ny = df1['CreditCard']\n\n# Split the dataset into train and test sections\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n# Train the logictic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train,y_train)\ny_pred=logisticRegr.predict(x_test)\n\n\n# Find model accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy : {0:0.3f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy : 0.748\n\n\nFinally, let us look at some metrics of our model.\n\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.74      0.99      0.85       704\n           1       0.88      0.17      0.29       296\n\n    accuracy                           0.75      1000\n   macro avg       0.81      0.58      0.57      1000\nweighted avg       0.78      0.75      0.68      1000\n\n\n\nThe model can further be made more accurate by tuning hyperparameters, which we will look in a separate blogpost.\nThank You!"
  },
  {
    "objectID": "posts/post-with-code/index.html#types-of-supervised-learning",
    "href": "posts/post-with-code/index.html#types-of-supervised-learning",
    "title": "Supervised Learning",
    "section": "",
    "text": "Classification: In classification , the goal is to assign input data to one of several predefined categories or classes. For example, spam email detection is a classification task where emails are classified as either spam or not spam.\nRegression: In regression tasks, the goal is to predict a continuous numerical value. For instance, predicting the price of a house based on its features like size, number of bedrooms, and location is a regression problem.\n\nCommon algorithms used in supervised learning include linear regression, logistic regression, decision trees, random forests, support vector machines, and various types of neural networks like feedforward neural networks and convolutional neural networks.\nIn this post, let’s dive deeper into how Logistic Regression works."
  },
  {
    "objectID": "posts/post-with-code/index.html#binary-logistic-regression",
    "href": "posts/post-with-code/index.html#binary-logistic-regression",
    "title": "Supervised Learning",
    "section": "",
    "text": "Despite its name, logistic regression is used for classification, not regression. It is used for binary classification tasks, where the goal is to predict one of two possible outcomes or classes (usually represented as 0 and 1). In order to find predicted probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1.\n\n\n\n\n\nLet us take an example of how Logistic Regression works. In this example, we are going to work with a dataset that contains details of a bank customers and we will try to predict if a customer is eligible for a Credit Card or not.\nTo begin with, let us import the necessary Python modules and load the dataset.\n\n# Importing Necessary Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Load the Universal Bank Data\ndf = pd.read_csv('/home/raghav/Desktop/ML_Blog/posts/post-with-code/bank.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nAge\nExperience\nIncome\nZIP Code\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n1\n25\n1\n49\n91107\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n45\n19\n34\n90089\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n39\n15\n11\n94720\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n35\n9\n100\n94112\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n5\n35\n8\n45\n91330\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nBefore we train our model, we need to preprocess the data. Let us look if any feature has NULL values for all the data points.\n\ndf.isnull().sum()\n\nID                    0\nAge                   0\nExperience            0\nIncome                0\nZIP Code              0\nFamily                0\nCCAvg                 0\nEducation             0\nMortgage              0\nPersonal Loan         0\nSecurities Account    0\nCD Account            0\nOnline                0\nCreditCard            0\ndtype: int64\n\n\nSince no feature has NULL or zero values, let us find other features which might not contribute in prediction. ID and ZIP Code should not ideally determine if a person is eligible for Credit Card, so we should remove them from our dataset.\n\ndf1 = df.drop([\"ID\",\"ZIP Code\"], axis = 1)\ndf1.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n25\n1\n49\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n45\n19\n34\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n39\n15\n11\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n35\n9\n100\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n35\n8\n45\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nLet us now visualize the data and try to see the correlation between Income, Experience and having a credit card through a scatter plot.\n\nzero_class = df1[df1.CreditCard==0]\none_class = df1[df1.CreditCard==1]\n\n# Income vs Experience scatter plot\nplt.xlabel('Income')\nplt.ylabel('Experience')\nplt.scatter(zero_class['Income'],zero_class['Experience'], color = 'green', marker='+')\nplt.scatter(one_class['Income'], one_class['Experience'], color = 'red', marker='.')\n\n&lt;matplotlib.collections.PathCollection at 0x7fc0922ded50&gt;\n\n\n\n\n\nEven though the data is now clean, the values of different features are not comparable at all. Let us normalize them so that all features are brought to a similar scale or range without distorting the relative differences between their values.\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled = scaler.fit(df1.drop('CreditCard',axis=1)).transform(df1.drop('CreditCard',axis=1))\ndf_scaled = pd.DataFrame(scaled, columns=df1.columns[:-1])\ndf_scaled.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\n\n\n\n\n0\n-1.774417\n-1.666078\n-0.538229\n1.397414\n-0.193371\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n1\n-0.029524\n-0.096330\n-0.864109\n0.525991\n-0.250595\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n2\n-0.552992\n-0.445163\n-1.363793\n-1.216855\n-0.536720\n-1.049078\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n3\n-0.901970\n-0.968413\n0.569765\n-1.216855\n0.436103\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n4\n-0.901970\n-1.055621\n-0.625130\n1.397414\n-0.536720\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n\n\n\n\n\nLet us finally jump into building our model.\n\n# Create X and Y for the labeled data.\nx = df_scaled\ny = df1['CreditCard']\n\n# Split the dataset into train and test sections\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n# Train the logictic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train,y_train)\ny_pred=logisticRegr.predict(x_test)\n\n\n# Find model accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy : {0:0.3f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy : 0.748\n\n\nFinally, let us look at some metrics of our model.\n\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.74      0.99      0.85       704\n           1       0.88      0.17      0.29       296\n\n    accuracy                           0.75      1000\n   macro avg       0.81      0.58      0.57      1000\nweighted avg       0.78      0.75      0.68      1000\n\n\n\nThe model can further be made more accurate by tuning hyperparameters, which we will look in a separate blogpost.\nThank You!"
  }
]