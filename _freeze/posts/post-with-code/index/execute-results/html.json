{
  "hash": "15dcc8abe4cece3fa21880c244583b29",
  "result": {
    "markdown": "---\ntitle: \"Supervised Learning\"\nauthor: \"Raghav Agrawal\"\ndate: \"2023-09-27\"\ncategories: [ml, supervised, logistic regression]\nimage: \"image.jpg\"\n---\n\n::: {.justify}\n\nSupervised learning is a type of machine learning where an algorithm learns from a labeled dataset, which means it is provided with input-output pairs (or examples) during training. The goal of supervised learning is to learn a mapping from input data to the corresponding output or target variable, so that the algorithm can make predictions or classify new, unseen data based on the patterns it has learned from the training data. Supervised learning is widely used in various domains, including natural language processing, computer vision, recommendation systems, and healthcare, among others, to solve a wide range of problems where the relationship between input and output can be learned from labeled data.\n\nHere are some key characteristics of supervised learning:\n\n1. **Labeled Data**: In supervised learning, you have a dataset where each data point is associated with a known output. These labels provide the ground truth for the algorithm to learn from.\n\n2. **Training Phase**: The algorithm goes through a training phase where it processes the labeled data to learn a model or function that can map input features to the correct output. The goal is to minimize the difference between the predicted outputs and the true labels.\n\n3. **Prediction**: After training, the model can be used to make predictions on new, unseen data. The model takes the input features of the new data and produces predictions or classifications based on what it has learned during training. \n\n![Supervised Learning](supervised1.png){width=\"80%\"}\n\n## Types of supervised learning\n\n1. **Classification**: In classification , the goal is to assign input data to one of several predefined categories or classes. For example, spam email detection is a classification task where emails are classified as either spam or not spam.\n    \n2. **Regression**: In regression tasks, the goal is to predict a continuous numerical value. For instance, predicting the price of a house based on its features like size, number of bedrooms, and location is a regression problem.\n\n\n\nCommon algorithms used in supervised learning include *linear regression, logistic regression, decision trees, random forests, support vector machines*, and various types of *neural networks* like feedforward neural networks and convolutional neural networks.\n\nIn this post, let's dive deeper into how **Logistic Regression** works.\n\n## Binary Logistic Regression\n\nDespite its name, logistic regression is used for classification, not regression. It is used for binary classification tasks, where the goal is to predict one of two possible outcomes or classes (usually represented as 0 and 1). In order to find predicted probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1.\n\n![](sigmoid.png){fig-align=\"center\" width=\"80%\"}\n\nLet us take an example of how Logistic Regression works. In this example, we are going to work with a dataset that contains details of a bank customers and we will try to predict if a customer is eligible for a Credit Card or not.\n\nTo begin with, let us import the necessary Python modules and load the dataset.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Importing Necessary Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Load the Universal Bank Data\ndf = pd.read_csv('/home/raghav/Desktop/ML_Blog/posts/post-with-code/bank.csv')\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Age</th>\n      <th>Experience</th>\n      <th>Income</th>\n      <th>ZIP Code</th>\n      <th>Family</th>\n      <th>CCAvg</th>\n      <th>Education</th>\n      <th>Mortgage</th>\n      <th>Personal Loan</th>\n      <th>Securities Account</th>\n      <th>CD Account</th>\n      <th>Online</th>\n      <th>CreditCard</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>25</td>\n      <td>1</td>\n      <td>49</td>\n      <td>91107</td>\n      <td>4</td>\n      <td>1.6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>45</td>\n      <td>19</td>\n      <td>34</td>\n      <td>90089</td>\n      <td>3</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>39</td>\n      <td>15</td>\n      <td>11</td>\n      <td>94720</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>35</td>\n      <td>9</td>\n      <td>100</td>\n      <td>94112</td>\n      <td>1</td>\n      <td>2.7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>35</td>\n      <td>8</td>\n      <td>45</td>\n      <td>91330</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBefore we train our model, we need to preprocess the data. Let us look if any feature has NULL values for all the data points.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nID                    0\nAge                   0\nExperience            0\nIncome                0\nZIP Code              0\nFamily                0\nCCAvg                 0\nEducation             0\nMortgage              0\nPersonal Loan         0\nSecurities Account    0\nCD Account            0\nOnline                0\nCreditCard            0\ndtype: int64\n```\n:::\n:::\n\n\nSince no feature has NULL or zero values, let us find other features which might not contribute in prediction. *ID* and *ZIP Code* should not ideally determine if a person is eligible for Credit Card, so we should remove them from our dataset. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf1 = df.drop([\"ID\",\"ZIP Code\"], axis = 1)\ndf1.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Experience</th>\n      <th>Income</th>\n      <th>Family</th>\n      <th>CCAvg</th>\n      <th>Education</th>\n      <th>Mortgage</th>\n      <th>Personal Loan</th>\n      <th>Securities Account</th>\n      <th>CD Account</th>\n      <th>Online</th>\n      <th>CreditCard</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>1</td>\n      <td>49</td>\n      <td>4</td>\n      <td>1.6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>45</td>\n      <td>19</td>\n      <td>34</td>\n      <td>3</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>39</td>\n      <td>15</td>\n      <td>11</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35</td>\n      <td>9</td>\n      <td>100</td>\n      <td>1</td>\n      <td>2.7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35</td>\n      <td>8</td>\n      <td>45</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet us now visualize the data and try to see the correlation between *Income*, *Experience* and *having a credit card* through a scatter plot.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nzero_class = df1[df1.CreditCard==0]\none_class = df1[df1.CreditCard==1]\n\n# Income vs Experience scatter plot\nplt.xlabel('Income')\nplt.ylabel('Experience')\nplt.scatter(zero_class['Income'],zero_class['Experience'], color = 'green', marker='+')\nplt.scatter(one_class['Income'], one_class['Experience'], color = 'red', marker='.')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<matplotlib.collections.PathCollection at 0x7fc0922ded50>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=585 height=429}\n:::\n:::\n\n\nEven though the data is now clean, the values of different features are not comparable at all. Let us normalize them so that all features are brought to a similar scale or range without distorting the relative differences between their values.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled = scaler.fit(df1.drop('CreditCard',axis=1)).transform(df1.drop('CreditCard',axis=1))\ndf_scaled = pd.DataFrame(scaled, columns=df1.columns[:-1])\ndf_scaled.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Experience</th>\n      <th>Income</th>\n      <th>Family</th>\n      <th>CCAvg</th>\n      <th>Education</th>\n      <th>Mortgage</th>\n      <th>Personal Loan</th>\n      <th>Securities Account</th>\n      <th>CD Account</th>\n      <th>Online</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.774417</td>\n      <td>-1.666078</td>\n      <td>-0.538229</td>\n      <td>1.397414</td>\n      <td>-0.193371</td>\n      <td>-1.049078</td>\n      <td>-0.555524</td>\n      <td>-0.325875</td>\n      <td>2.928915</td>\n      <td>-0.25354</td>\n      <td>-1.216618</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.029524</td>\n      <td>-0.096330</td>\n      <td>-0.864109</td>\n      <td>0.525991</td>\n      <td>-0.250595</td>\n      <td>-1.049078</td>\n      <td>-0.555524</td>\n      <td>-0.325875</td>\n      <td>2.928915</td>\n      <td>-0.25354</td>\n      <td>-1.216618</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.552992</td>\n      <td>-0.445163</td>\n      <td>-1.363793</td>\n      <td>-1.216855</td>\n      <td>-0.536720</td>\n      <td>-1.049078</td>\n      <td>-0.555524</td>\n      <td>-0.325875</td>\n      <td>-0.341423</td>\n      <td>-0.25354</td>\n      <td>-1.216618</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.901970</td>\n      <td>-0.968413</td>\n      <td>0.569765</td>\n      <td>-1.216855</td>\n      <td>0.436103</td>\n      <td>0.141703</td>\n      <td>-0.555524</td>\n      <td>-0.325875</td>\n      <td>-0.341423</td>\n      <td>-0.25354</td>\n      <td>-1.216618</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.901970</td>\n      <td>-1.055621</td>\n      <td>-0.625130</td>\n      <td>1.397414</td>\n      <td>-0.536720</td>\n      <td>0.141703</td>\n      <td>-0.555524</td>\n      <td>-0.325875</td>\n      <td>-0.341423</td>\n      <td>-0.25354</td>\n      <td>-1.216618</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet us finally jump into building our model.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Create X and Y for the labeled data.\nx = df_scaled\ny = df1['CreditCard']\n\n# Split the dataset into train and test sections\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n# Train the logictic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train,y_train)\ny_pred=logisticRegr.predict(x_test)\n\n\n# Find model accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy : {0:0.3f}'. format(accuracy_score(y_test, y_pred)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel accuracy : 0.748\n```\n:::\n:::\n\n\nFinally, let us look at some metrics of our model. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<AxesSubplot:>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=538 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.74      0.99      0.85       704\n           1       0.88      0.17      0.29       296\n\n    accuracy                           0.75      1000\n   macro avg       0.81      0.58      0.57      1000\nweighted avg       0.78      0.75      0.68      1000\n\n```\n:::\n:::\n\n\nThe model can further be made more accurate by tuning hyperparameters, which we will look in a separate blogpost.\n\nThank You!\n\n\n\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}