[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Raghav",
    "section": "",
    "text": "I am a Master’s student in the IT Security Lab at Virginia Tech. This is my first attempt towards learning Machine Learning. Hope this proves useful!"
  },
  {
    "objectID": "posts/SupervisedLearning/index.html",
    "href": "posts/SupervisedLearning/index.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised learning is a type of machine learning where an algorithm learns from a labeled dataset, which means it is provided with input-output pairs (or examples) during training. The goal of supervised learning is to learn a mapping from input data to the corresponding output or target variable, so that the algorithm can make predictions or classify new, unseen data based on the patterns it has learned from the training data. Supervised learning is widely used in various domains, including natural language processing, computer vision, recommendation systems, and healthcare, among others, to solve a wide range of problems where the relationship between input and output can be learned from labeled data.\nHere are some key characteristics of supervised learning:\n\nLabeled Data: In supervised learning, you have a dataset where each data point is associated with a known output. These labels provide the ground truth for the algorithm to learn from.\nTraining Phase: The algorithm goes through a training phase where it processes the labeled data to learn a model or function that can map input features to the correct output. The goal is to minimize the difference between the predicted outputs and the true labels.\nPrediction: After training, the model can be used to make predictions on new, unseen data. The model takes the input features of the new data and produces predictions or classifications based on what it has learned during training.\n\n\n\n\nSupervised Learning\n\n\n\n\n\nClassification: In classification , the goal is to assign input data to one of several predefined categories or classes. For example, spam email detection is a classification task where emails are classified as either spam or not spam.\nRegression: In regression tasks, the goal is to predict a continuous numerical value. For instance, predicting the price of a house based on its features like size, number of bedrooms, and location is a regression problem.\n\nCommon algorithms used in supervised learning include linear regression, logistic regression, decision trees, random forests, support vector machines, and various types of neural networks like feedforward neural networks and convolutional neural networks.\nIn this post, let’s dive deeper into how Logistic Regression works.\n\n\n\nDespite its name, logistic regression is used for classification, not regression. It is used for binary classification tasks, where the goal is to predict one of two possible outcomes or classes (usually represented as 0 and 1). In order to find predicted probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1.\n\n\n\n\n\nLet us take an example of how Logistic Regression works. In this example, we are going to work with a dataset that contains details of a bank customers and we will try to predict if a customer is eligible for a Credit Card or not.\nTo begin with, let us import the necessary Python modules and load the dataset.\n\n# Importing Necessary Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Load the Universal Bank Data\ndf = pd.read_csv('bank.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nAge\nExperience\nIncome\nZIP Code\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n1\n25\n1\n49\n91107\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n45\n19\n34\n90089\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n39\n15\n11\n94720\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n35\n9\n100\n94112\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n5\n35\n8\n45\n91330\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nBefore we train our model, we need to preprocess the data. Let us look if any feature has NULL values for all the data points.\n\ndf.isnull().sum()\n\nID                    0\nAge                   0\nExperience            0\nIncome                0\nZIP Code              0\nFamily                0\nCCAvg                 0\nEducation             0\nMortgage              0\nPersonal Loan         0\nSecurities Account    0\nCD Account            0\nOnline                0\nCreditCard            0\ndtype: int64\n\n\nSince no feature has NULL or zero values, let us find other features which might not contribute in prediction. ID and ZIP Code should not ideally determine if a person is eligible for Credit Card, so we should remove them from our dataset.\n\ndf1 = df.drop([\"ID\",\"ZIP Code\"], axis = 1)\ndf1.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n25\n1\n49\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n45\n19\n34\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n39\n15\n11\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n35\n9\n100\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n35\n8\n45\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nLet us now visualize the data and try to see the correlation between Income, Experience and having a credit card through a scatter plot.\n\nzero_class = df1[df1.CreditCard==0]\none_class = df1[df1.CreditCard==1]\n\n# Income vs Experience scatter plot\nplt.xlabel('Income')\nplt.ylabel('Experience')\nplt.scatter(zero_class['Income'],zero_class['Experience'], color = 'green', marker='+')\nplt.scatter(one_class['Income'], one_class['Experience'], color = 'red', marker='.')\n\n&lt;matplotlib.collections.PathCollection at 0x7f311208e5d0&gt;\n\n\n\n\n\nEven though the data is now clean, the values of different features are not comparable at all. Let us normalize them so that all features are brought to a similar scale or range without distorting the relative differences between their values.\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled = scaler.fit(df1.drop('CreditCard',axis=1)).transform(df1.drop('CreditCard',axis=1))\ndf_scaled = pd.DataFrame(scaled, columns=df1.columns[:-1])\ndf_scaled.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\n\n\n\n\n0\n-1.774417\n-1.666078\n-0.538229\n1.397414\n-0.193371\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n1\n-0.029524\n-0.096330\n-0.864109\n0.525991\n-0.250595\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n2\n-0.552992\n-0.445163\n-1.363793\n-1.216855\n-0.536720\n-1.049078\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n3\n-0.901970\n-0.968413\n0.569765\n-1.216855\n0.436103\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n4\n-0.901970\n-1.055621\n-0.625130\n1.397414\n-0.536720\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n\n\n\n\n\nLet us finally jump into building our model.\n\n# Create X and Y for the labeled data.\nx = df_scaled\ny = df1['CreditCard']\n\n# Split the dataset into train and test sections\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n# Train the logictic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train,y_train)\ny_pred=logisticRegr.predict(x_test)\n\n\n# Find model accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy : {0:0.3f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy : 0.748\n\n\nFinally, let us look at some metrics of our model.\n\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.74      0.99      0.85       704\n           1       0.88      0.17      0.29       296\n\n    accuracy                           0.75      1000\n   macro avg       0.81      0.58      0.57      1000\nweighted avg       0.78      0.75      0.68      1000\n\n\n\nThe model can further be made more accurate by tuning hyperparameters, which we will look in a separate blogpost.\nThank You!"
  },
  {
    "objectID": "posts/SupervisedLearning/index.html#types-of-supervised-learning",
    "href": "posts/SupervisedLearning/index.html#types-of-supervised-learning",
    "title": "Supervised Learning",
    "section": "",
    "text": "Classification: In classification , the goal is to assign input data to one of several predefined categories or classes. For example, spam email detection is a classification task where emails are classified as either spam or not spam.\nRegression: In regression tasks, the goal is to predict a continuous numerical value. For instance, predicting the price of a house based on its features like size, number of bedrooms, and location is a regression problem.\n\nCommon algorithms used in supervised learning include linear regression, logistic regression, decision trees, random forests, support vector machines, and various types of neural networks like feedforward neural networks and convolutional neural networks.\nIn this post, let’s dive deeper into how Logistic Regression works."
  },
  {
    "objectID": "posts/SupervisedLearning/index.html#binary-logistic-regression",
    "href": "posts/SupervisedLearning/index.html#binary-logistic-regression",
    "title": "Supervised Learning",
    "section": "",
    "text": "Despite its name, logistic regression is used for classification, not regression. It is used for binary classification tasks, where the goal is to predict one of two possible outcomes or classes (usually represented as 0 and 1). In order to find predicted probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1.\n\n\n\n\n\nLet us take an example of how Logistic Regression works. In this example, we are going to work with a dataset that contains details of a bank customers and we will try to predict if a customer is eligible for a Credit Card or not.\nTo begin with, let us import the necessary Python modules and load the dataset.\n\n# Importing Necessary Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Load the Universal Bank Data\ndf = pd.read_csv('bank.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nAge\nExperience\nIncome\nZIP Code\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n1\n25\n1\n49\n91107\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n45\n19\n34\n90089\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n39\n15\n11\n94720\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n35\n9\n100\n94112\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n5\n35\n8\n45\n91330\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nBefore we train our model, we need to preprocess the data. Let us look if any feature has NULL values for all the data points.\n\ndf.isnull().sum()\n\nID                    0\nAge                   0\nExperience            0\nIncome                0\nZIP Code              0\nFamily                0\nCCAvg                 0\nEducation             0\nMortgage              0\nPersonal Loan         0\nSecurities Account    0\nCD Account            0\nOnline                0\nCreditCard            0\ndtype: int64\n\n\nSince no feature has NULL or zero values, let us find other features which might not contribute in prediction. ID and ZIP Code should not ideally determine if a person is eligible for Credit Card, so we should remove them from our dataset.\n\ndf1 = df.drop([\"ID\",\"ZIP Code\"], axis = 1)\ndf1.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n25\n1\n49\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n45\n19\n34\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n39\n15\n11\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n35\n9\n100\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n35\n8\n45\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nLet us now visualize the data and try to see the correlation between Income, Experience and having a credit card through a scatter plot.\n\nzero_class = df1[df1.CreditCard==0]\none_class = df1[df1.CreditCard==1]\n\n# Income vs Experience scatter plot\nplt.xlabel('Income')\nplt.ylabel('Experience')\nplt.scatter(zero_class['Income'],zero_class['Experience'], color = 'green', marker='+')\nplt.scatter(one_class['Income'], one_class['Experience'], color = 'red', marker='.')\n\n&lt;matplotlib.collections.PathCollection at 0x7f311208e5d0&gt;\n\n\n\n\n\nEven though the data is now clean, the values of different features are not comparable at all. Let us normalize them so that all features are brought to a similar scale or range without distorting the relative differences between their values.\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled = scaler.fit(df1.drop('CreditCard',axis=1)).transform(df1.drop('CreditCard',axis=1))\ndf_scaled = pd.DataFrame(scaled, columns=df1.columns[:-1])\ndf_scaled.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\n\n\n\n\n0\n-1.774417\n-1.666078\n-0.538229\n1.397414\n-0.193371\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n1\n-0.029524\n-0.096330\n-0.864109\n0.525991\n-0.250595\n-1.049078\n-0.555524\n-0.325875\n2.928915\n-0.25354\n-1.216618\n\n\n2\n-0.552992\n-0.445163\n-1.363793\n-1.216855\n-0.536720\n-1.049078\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n3\n-0.901970\n-0.968413\n0.569765\n-1.216855\n0.436103\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n4\n-0.901970\n-1.055621\n-0.625130\n1.397414\n-0.536720\n0.141703\n-0.555524\n-0.325875\n-0.341423\n-0.25354\n-1.216618\n\n\n\n\n\n\n\nLet us finally jump into building our model.\n\n# Create X and Y for the labeled data.\nx = df_scaled\ny = df1['CreditCard']\n\n# Split the dataset into train and test sections\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n# Train the logictic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train,y_train)\ny_pred=logisticRegr.predict(x_test)\n\n\n# Find model accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy : {0:0.3f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy : 0.748\n\n\nFinally, let us look at some metrics of our model.\n\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.74      0.99      0.85       704\n           1       0.88      0.17      0.29       296\n\n    accuracy                           0.75      1000\n   macro avg       0.81      0.58      0.57      1000\nweighted avg       0.78      0.75      0.68      1000\n\n\n\nThe model can further be made more accurate by tuning hyperparameters, which we will look in a separate blogpost.\nThank You!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning How Machines Learn",
    "section": "",
    "text": "Unsupervised Learning\n\n\n\n\n\n\n\nunsupervised\n\n\nk-means\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nRaghav Agrawal\n\n\n\n\n\n\n  \n\n\n\n\nSupervised Learning\n\n\n\n\n\n\n\nml\n\n\nsupervised\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nRaghav Agrawal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/UnsupervisedLearning/index.html",
    "href": "posts/UnsupervisedLearning/index.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning is a type of machine learning where an algorithm is trained on a dataset without labeled outputs or target values. The primary goal of unsupervised learning is to discover the inherent structure or organization in the data. It can be used for various tasks, including:\nClustering : Unsupervised learning algorithms can group data points into clusters based on similarities or shared characteristics. K-means clustering and hierarchical clustering are common techniques used for this purpose.\nDimensionality Reduction : Unsupervised learning can reduce the dimensionality of data by extracting relevant features or reducing noise. Principal Component Analysis (PCA) and t-SNE (t-distributed Stochastic Neighbor Embedding) are examples of dimensionality reduction techniques.\nAnomaly Detection: Unsupervised learning can identify unusual or rare data points, which may indicate anomalies or outliers in the dataset.\nUnsupervised learning is more exploratory in nature, as it doesn’t have specific target outcomes to achieve. It is commonly used in data analysis, pattern recognition, and data preprocessing to gain insights and structure from unlabelled data. Some of the most well-known unsupervised learning algorithms include:\nThese algorithms serve different purposes and are applied in a wide range of applications, from customer segmentation and image compression to outlier detection and data visualization. The choice of algorithm depends on the specific problem and the characteristics of the dataset you are working with.\nNow lets take an example of customer segmentation using K-means clustering."
  },
  {
    "objectID": "posts/UnsupervisedLearning/index.html#customer-segmentation-with-k-means-cluster",
    "href": "posts/UnsupervisedLearning/index.html#customer-segmentation-with-k-means-cluster",
    "title": "Unsupervised Learning",
    "section": "Customer Segmentation with K-means cluster",
    "text": "Customer Segmentation with K-means cluster\nWe have a dataset with customer information in a shopping mall. Using K-means clustering, we will try to find relationship between different attributes of a customer and how much they spend.\nFirst of all, let us import the necessary classes and load the dataset.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndb = pd.read_csv('Mall_Customers.csv')\ndb.head()\n\n\n\n\n\n\n\n\nID\nGender\nAge\nIncome (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\nSince most of the data in our set is numeric, expect the Gender, let us convert that to a numeric value as well.\n\ndb[\"Gender\"] = db.apply(lambda x: int(1) if ((x['Gender'])==\"Male\") else int(0), axis=1)\n\ndb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype\n---  ------                  --------------  -----\n 0   ID                      200 non-null    int64\n 1   Gender                  200 non-null    int64\n 2   Age                     200 non-null    int64\n 3   Income (k$)             200 non-null    int64\n 4   Spending Score (1-100)  200 non-null    int64\ndtypes: int64(5)\nmemory usage: 7.9 KB\n\n\nSo there are 200 non-null observations and no categorical variables anymore. All good, now what I want is to cluster the customers, and for this I’ll firstly choose to keep a database that finds relationship between income and spendings.\n\nincome_db = db.iloc[:,[3,4]]\nincome_db.head()\n\n\n\n\n\n\n\n\nIncome (k$)\nSpending Score (1-100)\n\n\n\n\n0\n15\n39\n\n\n1\n15\n81\n\n\n2\n16\n6\n\n\n3\n16\n77\n\n\n4\n17\n40\n\n\n\n\n\n\n\nThe next step is to determine how many clusters are optimal; for this let’s use the Elbow method, testing number of clusters between 1 and 10.\n\ncl = []\nlist_k = list(range(1, 10))\n\nfor k in list_k:\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(income_db)\n    cl.append(kmeans.inertia_)\n\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, cl, '-o')\nplt.xlabel(r'# Clusters')\nplt.ylabel('Sum of squared distance')\n\nText(0, 0.5, 'Sum of squared distance')\n\n\n\n\n\nThe idea behind the elbow method is to find a balance between minimizing the within-cluster variance (making clusters more homogeneous) and avoiding an excessive number of clusters that might overfit the data. The elbow point indicates the number of clusters at which the improvement in the chosen metric starts to slow down significantly, suggesting that adding more clusters does not yield much better clustering quality. The elbow of the graph is at 5, so let us create 5 clusters.\n\nkmeans = KMeans(n_clusters=5) \nkmeans.fit(income_db)\n\ny_km = kmeans.fit_predict(income_db)\n\nincome_db = np.array(income_db)\n\nplt.title(\"Clusters\", fontsize=20)\nplt.xlabel(\"Annual Income\")\nplt.ylabel(\"Spending Score\")\n\nplt.scatter(income_db[y_km ==0,0], income_db[y_km == 0,1], c='red')\nplt.scatter(income_db[y_km ==1,0], income_db[y_km == 1,1], c='black')\nplt.scatter(income_db[y_km ==2,0], income_db[y_km == 2,1],  c='blue')\nplt.scatter(income_db[y_km ==3,0], income_db[y_km == 3,1],  c='orange')\nplt.scatter(income_db[y_km ==4,0], income_db[y_km == 4,1],  c='yellow')\n\n&lt;matplotlib.collections.PathCollection at 0x7efb8befb010&gt;\n\n\n\n\n\nWe can easily distinguish our five clusters. Based on this distinction, we can categorize our clients into the following groups: individuals with lower incomes fall into two categories, some of whom are frugal in their spending , while others are more extravagant. Similarly, those with higher incomes can be divided in a comparable manner: a cluster consists of individuals who maintain modest spending habits, while another cluster represents those who tend to spend liberally. Additionally, there is a middle category comprising individuals with moderate incomes who exhibit moderate spending levels.\nNow lets us try to find correlations between Gender and Spendings.\n\ngender_db = db.iloc[:,[1,4]]\ngender_db.head()\ncl = []\nlist_k = list(range(1, 10))\n\nfor k in list_k:\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(gender_db)\n    cl.append(kmeans.inertia_)\n\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, cl, '-o')\nplt.xlabel(r'# Clusters')\nplt.ylabel('Sum of squared distance')\n\nText(0, 0.5, 'Sum of squared distance')\n\n\n\n\n\nSince the elbow of the curve is at three now, let us form 3 clusters.\n\nkmeans = KMeans(n_clusters=3) \nkmeans.fit(gender_db)\n\ny_km = kmeans.fit_predict(gender_db)\n\ngender_db = np.array(gender_db)\n\nplt.title(\"Clusters\", fontsize=20)\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Spending Score\")\n\nplt.scatter(gender_db[y_km ==0,0], gender_db[y_km == 0,1], c='red')\nplt.scatter(gender_db[y_km ==1,0], gender_db[y_km == 1,1], c='black')\nplt.scatter(gender_db[y_km ==2,0], gender_db[y_km == 2,1],  c='blue')\n\n&lt;matplotlib.collections.PathCollection at 0x7efc4c2bd3d0&gt;\n\n\n\n\n\nThe data between Males and Females seem to be a uniform split, and hence we don’t get much imformation froom this cluster.\nFinally, let us try to find a relationship between Age and Spendings.\n\nage_db = db.iloc[:,[2,4]]\nage_db.head()\n\ncl = []\nlist_k = list(range(1, 10))\n\nfor k in list_k:\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(age_db)\n    cl.append(kmeans.inertia_)\n\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, cl, '-o')\nplt.xlabel(r'# Clusters')\nplt.ylabel('Sum of squared distance')\n\nText(0, 0.5, 'Sum of squared distance')\n\n\n\n\n\nNow let us take 4 clusters.\n\nkmeans = KMeans(n_clusters=4) \nkmeans.fit(age_db)\n\ny_km = kmeans.fit_predict(age_db)\n\nage_db = np.array(age_db)\n\nplt.title(\"Clusters\", fontsize=20)\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Spending Score\")\n\nplt.scatter(age_db[y_km ==0,0], age_db[y_km == 0,1], c='red')\nplt.scatter(age_db[y_km ==1,0], age_db[y_km == 1,1], c='black')\nplt.scatter(age_db[y_km ==2,0], age_db[y_km == 2,1], c='blue')\nplt.scatter(age_db[y_km ==3,0], age_db[y_km == 3,1], c='orange')\n\n&lt;matplotlib.collections.PathCollection at 0x7efb8cc3b790&gt;\n\n\n\n\n\nAlright, we can readily distinguish a set of individuals who maintain low spending levels, with most of them being over 30 years old. Among those under 30, we can further categorize them into two groups: those who exhibit moderate spending habits and those who are more extravagant.\nHence, we can conclude that Customer segmentation with K-means clustering helps businesses better understand their customer base, improve customer satisfaction, and increase the effectiveness of marketing efforts by tailoring strategies to the unique needs of each segment. It’s a valuable tool for businesses in various industries, such as retail, e-commerce, finance, and more."
  }
]