{"title":"Supervised Learning","markdown":{"yaml":{"title":"Supervised Learning","author":"Raghav Agrawal","date":"2023-09-27","categories":["ml","supervised","logistic regression"],"image":"image.jpg"},"headingText":"Types of supervised learning","containsRefs":false,"markdown":"\n\n::: {.justify}\n\nSupervised learning is a type of machine learning where an algorithm learns from a labeled dataset, which means it is provided with input-output pairs (or examples) during training. The goal of supervised learning is to learn a mapping from input data to the corresponding output or target variable, so that the algorithm can make predictions or classify new, unseen data based on the patterns it has learned from the training data. Supervised learning is widely used in various domains, including natural language processing, computer vision, recommendation systems, and healthcare, among others, to solve a wide range of problems where the relationship between input and output can be learned from labeled data.\n\nHere are some key characteristics of supervised learning:\n\n1. **Labeled Data**: In supervised learning, you have a dataset where each data point is associated with a known output. These labels provide the ground truth for the algorithm to learn from.\n\n2. **Training Phase**: The algorithm goes through a training phase where it processes the labeled data to learn a model or function that can map input features to the correct output. The goal is to minimize the difference between the predicted outputs and the true labels.\n\n3. **Prediction**: After training, the model can be used to make predictions on new, unseen data. The model takes the input features of the new data and produces predictions or classifications based on what it has learned during training. \n\n![Supervised Learning](supervised1.png){width=\"80%\"}\n\n\n1. **Classification**: In classification , the goal is to assign input data to one of several predefined categories or classes. For example, spam email detection is a classification task where emails are classified as either spam or not spam.\n    \n2. **Regression**: In regression tasks, the goal is to predict a continuous numerical value. For instance, predicting the price of a house based on its features like size, number of bedrooms, and location is a regression problem.\n\n\n\nCommon algorithms used in supervised learning include *linear regression, logistic regression, decision trees, random forests, support vector machines*, and various types of *neural networks* like feedforward neural networks and convolutional neural networks.\n\nIn this post, let's dive deeper into how **Logistic Regression** works.\n\n## Binary Logistic Regression\n\nDespite its name, logistic regression is used for classification, not regression. It is used for binary classification tasks, where the goal is to predict one of two possible outcomes or classes (usually represented as 0 and 1). In order to find predicted probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1.\n\n![](sigmoid.png){fig-align=\"center\" width=\"80%\"}\n\nLet us take an example of how Logistic Regression works. In this example, we are going to work with a dataset that contains details of a bank customers and we will try to predict if a customer is eligible for a Credit Card or not.\n\nTo begin with, let us import the necessary Python modules and load the dataset.\n\n```{python}\n# Importing Necessary Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Load the Universal Bank Data\ndf = pd.read_csv('/home/raghav/Desktop/ML_Blog/posts/post-with-code/bank.csv')\ndf.head()\n```\n\n\nBefore we train our model, we need to preprocess the data. Let us look if any feature has NULL values for all the data points.\n\n```{python}\ndf.isnull().sum()\n```\n\nSince no feature has NULL or zero values, let us find other features which might not contribute in prediction. *ID* and *ZIP Code* should not ideally determine if a person is eligible for Credit Card, so we should remove them from our dataset. \n\n```{python}\ndf1 = df.drop([\"ID\",\"ZIP Code\"], axis = 1)\ndf1.head()\n```\n\n\nLet us now visualize the data and try to see the correlation between *Income*, *Experience* and *having a credit card* through a scatter plot.\n\n```{python}\nzero_class = df1[df1.CreditCard==0]\none_class = df1[df1.CreditCard==1]\n\n# Income vs Experience scatter plot\nplt.xlabel('Income')\nplt.ylabel('Experience')\nplt.scatter(zero_class['Income'],zero_class['Experience'], color = 'green', marker='+')\nplt.scatter(one_class['Income'], one_class['Experience'], color = 'red', marker='.')\n```\n\nEven though the data is now clean, the values of different features are not comparable at all. Let us normalize them so that all features are brought to a similar scale or range without distorting the relative differences between their values.\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled = scaler.fit(df1.drop('CreditCard',axis=1)).transform(df1.drop('CreditCard',axis=1))\ndf_scaled = pd.DataFrame(scaled, columns=df1.columns[:-1])\ndf_scaled.head()\n```\n\n\nLet us finally jump into building our model.\n```{python}\n# Create X and Y for the labeled data.\nx = df_scaled\ny = df1['CreditCard']\n\n# Split the dataset into train and test sections\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n# Train the logictic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train,y_train)\ny_pred=logisticRegr.predict(x_test)\n\n\n# Find model accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy : {0:0.3f}'. format(accuracy_score(y_test, y_pred)))\n\n```\n\nFinally, let us look at some metrics of our model. \n\n```{python}\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')\n```\n\n```{python}\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n```\n\n\nThe model can further be made more accurate by tuning hyperparameters, which we will look in a separate blogpost.\n\nThank You!\n\n\n\n:::","srcMarkdownNoYaml":"\n\n::: {.justify}\n\nSupervised learning is a type of machine learning where an algorithm learns from a labeled dataset, which means it is provided with input-output pairs (or examples) during training. The goal of supervised learning is to learn a mapping from input data to the corresponding output or target variable, so that the algorithm can make predictions or classify new, unseen data based on the patterns it has learned from the training data. Supervised learning is widely used in various domains, including natural language processing, computer vision, recommendation systems, and healthcare, among others, to solve a wide range of problems where the relationship between input and output can be learned from labeled data.\n\nHere are some key characteristics of supervised learning:\n\n1. **Labeled Data**: In supervised learning, you have a dataset where each data point is associated with a known output. These labels provide the ground truth for the algorithm to learn from.\n\n2. **Training Phase**: The algorithm goes through a training phase where it processes the labeled data to learn a model or function that can map input features to the correct output. The goal is to minimize the difference between the predicted outputs and the true labels.\n\n3. **Prediction**: After training, the model can be used to make predictions on new, unseen data. The model takes the input features of the new data and produces predictions or classifications based on what it has learned during training. \n\n![Supervised Learning](supervised1.png){width=\"80%\"}\n\n## Types of supervised learning\n\n1. **Classification**: In classification , the goal is to assign input data to one of several predefined categories or classes. For example, spam email detection is a classification task where emails are classified as either spam or not spam.\n    \n2. **Regression**: In regression tasks, the goal is to predict a continuous numerical value. For instance, predicting the price of a house based on its features like size, number of bedrooms, and location is a regression problem.\n\n\n\nCommon algorithms used in supervised learning include *linear regression, logistic regression, decision trees, random forests, support vector machines*, and various types of *neural networks* like feedforward neural networks and convolutional neural networks.\n\nIn this post, let's dive deeper into how **Logistic Regression** works.\n\n## Binary Logistic Regression\n\nDespite its name, logistic regression is used for classification, not regression. It is used for binary classification tasks, where the goal is to predict one of two possible outcomes or classes (usually represented as 0 and 1). In order to find predicted probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1.\n\n![](sigmoid.png){fig-align=\"center\" width=\"80%\"}\n\nLet us take an example of how Logistic Regression works. In this example, we are going to work with a dataset that contains details of a bank customers and we will try to predict if a customer is eligible for a Credit Card or not.\n\nTo begin with, let us import the necessary Python modules and load the dataset.\n\n```{python}\n# Importing Necessary Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Load the Universal Bank Data\ndf = pd.read_csv('/home/raghav/Desktop/ML_Blog/posts/post-with-code/bank.csv')\ndf.head()\n```\n\n\nBefore we train our model, we need to preprocess the data. Let us look if any feature has NULL values for all the data points.\n\n```{python}\ndf.isnull().sum()\n```\n\nSince no feature has NULL or zero values, let us find other features which might not contribute in prediction. *ID* and *ZIP Code* should not ideally determine if a person is eligible for Credit Card, so we should remove them from our dataset. \n\n```{python}\ndf1 = df.drop([\"ID\",\"ZIP Code\"], axis = 1)\ndf1.head()\n```\n\n\nLet us now visualize the data and try to see the correlation between *Income*, *Experience* and *having a credit card* through a scatter plot.\n\n```{python}\nzero_class = df1[df1.CreditCard==0]\none_class = df1[df1.CreditCard==1]\n\n# Income vs Experience scatter plot\nplt.xlabel('Income')\nplt.ylabel('Experience')\nplt.scatter(zero_class['Income'],zero_class['Experience'], color = 'green', marker='+')\nplt.scatter(one_class['Income'], one_class['Experience'], color = 'red', marker='.')\n```\n\nEven though the data is now clean, the values of different features are not comparable at all. Let us normalize them so that all features are brought to a similar scale or range without distorting the relative differences between their values.\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled = scaler.fit(df1.drop('CreditCard',axis=1)).transform(df1.drop('CreditCard',axis=1))\ndf_scaled = pd.DataFrame(scaled, columns=df1.columns[:-1])\ndf_scaled.head()\n```\n\n\nLet us finally jump into building our model.\n```{python}\n# Create X and Y for the labeled data.\nx = df_scaled\ny = df1['CreditCard']\n\n# Split the dataset into train and test sections\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n# Train the logictic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train,y_train)\ny_pred=logisticRegr.predict(x_test)\n\n\n# Find model accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy : {0:0.3f}'. format(accuracy_score(y_test, y_pred)))\n\n```\n\nFinally, let us look at some metrics of our model. \n\n```{python}\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')\n```\n\n```{python}\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n```\n\n\nThe model can further be made more accurate by tuning hyperparameters, which we will look in a separate blogpost.\n\nThank You!\n\n\n\n:::"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"solar","title-block-banner":true,"title":"Supervised Learning","author":"Raghav Agrawal","date":"2023-09-27","categories":["ml","supervised","logistic regression"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}